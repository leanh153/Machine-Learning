{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cnD1ecF0Mw0u"
   },
   "source": [
    "# Feature Selection and LASSO (Interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6oYlO_fMw0z"
   },
   "source": [
    "In this notebook, you will use LASSO to select features, building on a pre-implemented solver for LASSO (in sklearn, obviously). You will:\n",
    "* Run LASSO with different L1 penalties.\n",
    "* Choose best L1 penalty using a validation set.\n",
    "* Choose best L1 penalty using a validation set, with additional constraint on the size of subset.\n",
    "\n",
    "In the next exercise, you will implement your own LASSO solver, using coordinate descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wRSBur8HMw02"
   },
   "source": [
    "## The usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MjaAEidBMw05"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JBmiSN8sMw1A"
   },
   "source": [
    "## Load in house sales data\n",
    "\n",
    "Dataset is from house sales in King County, the region where the city of Seattle, WA is located. *I am so surprised.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7cDLiqAIMw1D",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_data = pd.read_csv(\"kc_house_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nyLeYGvlMw1J"
   },
   "source": [
    "## Create new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XSMOdbHVMw1L"
   },
   "source": [
    "As in Lab 2 (*lab-2.ipynb*), we consider features that are some transformations of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mYzffDTvMw1M"
   },
   "outputs": [],
   "source": [
    "from math import log, sqrt\n",
    "full_data['sqft_living_sqrt'] = full_data['sqft_living'].map(sqrt)\n",
    "full_data['sqft_lot_sqrt'] = full_data['sqft_lot'].map(sqrt)\n",
    "full_data['bedrooms_square'] = full_data['bedrooms'] ** 2\n",
    "\n",
    "# In the dataset, 'floors' was defined with type string, \n",
    "# so we'll convert them to float, before creating a new feature.\n",
    "full_data['floors'] = full_data['floors'].astype(float) \n",
    "full_data['floors_square'] = full_data['floors'] ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z672nh0kMw1S"
   },
   "source": [
    "* Squaring bedrooms will increase the separation between not many bedrooms (e.g. 1) and lots of bedrooms (e.g. 4) since 1^2 = 1 but 4^2 = 16. Consequently this variable will mostly affect houses with many bedrooms.\n",
    "* On the other hand, taking square root of sqft_living will decrease the separation between big house and small house. The owner may not be exactly twice as happy for getting a house that is twice as big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VDo2w5chMw1U"
   },
   "source": [
    "# Learn regression weights with L1 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u-xMl9bUMw1V"
   },
   "source": [
    "Let us fit a model with all the features available, plus the features we just created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OYAUT1PLMw1W"
   },
   "outputs": [],
   "source": [
    "all_features = ['bedrooms', 'bedrooms_square',\n",
    "            'bathrooms',\n",
    "            'sqft_living', 'sqft_living_sqrt',\n",
    "            'sqft_lot', 'sqft_lot_sqrt',\n",
    "            'floors', 'floors_square',\n",
    "            'waterfront', 'view', 'condition', 'grade',\n",
    "            'sqft_above',\n",
    "            'sqft_basement',\n",
    "            'yr_built', 'yr_renovated']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7-592ikMw1e"
   },
   "source": [
    "Applying L1 penalty requires adding an extra parameter (`alpha=l1_penalty`) to the sklearn model `Lasso`. (Other tools may have separate implementations of LASSO). Much like L2/Ridge Regression, the features should be scaled to ensure equal attention inbetween."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KlTbDC0RMw1g"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "l1_penalty=5e4\n",
    "full_features = scaler.fit_transform(full_data[all_features].values)\n",
    "full_labels = full_data['price'].values\n",
    "model = Lasso(alpha=l1_penalty).fit(full_features, full_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=50000.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "      normalize=False, positive=False, precompute=False, random_state=None,\n",
       "      selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rH4847CcMw1m"
   },
   "source": [
    "Find what features had non-zero weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HElqPj8LMw1n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sqft_living', 'waterfront', 'view', 'grade', 'yr_built'],\n",
       "      dtype='<U16')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do you know that even numpy has built-in boolean selector?\n",
    "np.array(all_features)[model.coef_ != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540088.1417665294"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0gF5MD_Mw1r"
   },
   "source": [
    "Note that a majority of the weights have been set to zero. So by setting an L1 penalty that's large enough, we are performing a subset selection. \n",
    "\n",
    "***QUIZ QUESTION***:\n",
    "According to this list of weights, which of the features have been chosen? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Zv3CFZBMw1s"
   },
   "source": [
    "# Selecting an L1 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oc6Vm3RfMw1t"
   },
   "source": [
    "To find a good L1 penalty, we will explore multiple values using a validation set. Let us do three way split into train, validation, and test sets:\n",
    "* Split our sales data into 2 sets: training and test\n",
    "* Further split our training data into two sets: train, validation\n",
    "\n",
    "Be *very* careful that you use seed = 1 to ensure you get the same answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Cookie for those who instantly remember what cell to copy\n",
    "train_validation_set, test_set = train_test_split(full_data, train_size=0.9, test_size=0.1, random_state=1)\n",
    "train_set, validation_set = train_test_split(train_validation_set, train_size=0.5, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7kCoG3VwMw10"
   },
   "source": [
    "Next, we write a loop that does the following:\n",
    "* For `l1_penalty` in 21 steps range between [1, 10^9] (use `np.logspace(0, 9, num=21)`.)\n",
    "    * Fit a regression model with a given `l1_penalty` on TRAIN data. Specify `alpha=l1_penalty` in the parameter.\n",
    "    * Compute the RSS on VALIDATION data (here you will want to use `.predict()`) for that `l1_penalty`\n",
    "* Report which `l1_penalty` produced the lowest RSS on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kYf8zDJtMw11"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leanh/anaconda3/envs/ml-301x/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14654475719675.5, tolerance: 127509489470.134\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leanh/anaconda3/envs/ml-301x/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14663181882952.375, tolerance: 127509489470.134\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=2.8183829312644537, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leanh/anaconda3/envs/ml-301x/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14687653596241.812, tolerance: 127509489470.134\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=7.943282347242816, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leanh/anaconda3/envs/ml-301x/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14755977786894.688, tolerance: 127509489470.134\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=22.3872113856834, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leanh/anaconda3/envs/ml-301x/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14943361906599.0, tolerance: 127509489470.134\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=63.09573444801933, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leanh/anaconda3/envs/ml-301x/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15430211624777.625, tolerance: 127509489470.134\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=177.82794100389228, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leanh/anaconda3/envs/ml-301x/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 16474937844499.812, tolerance: 127509489470.134\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=501.18723362727246, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=1412.537544622754, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=3981.0717055349733, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=11220.18454301963, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=31622.776601683792, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=89125.0938133746, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=251188.6431509582, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=707945.7843841388, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=1995262.3149688789, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=5623413.251903491, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=15848931.924611142, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=44668359.21509635, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=125892541.17941661, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=354813389.23357606, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=1000000000.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# Handholding level: Nominally.\n",
    "# get features and labels on training set\n",
    "X_train_features = scaler.transform(train_set[all_features].values)\n",
    "y_train_label = train_set['price'].values\n",
    "# get features and labels on validation set\n",
    "X_validation_features = scaler.transform(validation_set[all_features].values)\n",
    "y_validation_label = validation_set['price'].values\n",
    "return_data_list = []\n",
    "for l1_penalty in np.logspace(0, 9, 21):\n",
    "    model = Lasso(alpha=l1_penalty).fit(X_train_features, y_train_label)\n",
    "    y_predict = model.predict(X_validation_features)\n",
    "    rss = np.sum(np.square(y_validation_label - y_predict))\n",
    "    return_data_list.append([rss, l1_penalty, model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wAneHq6LMw15"
   },
   "source": [
    "*** QUIZ QUESTION. *** What was the best value for the `l1_penalty`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc19516a5d0>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "l = np.array(return_data_list)\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"RSS\")\n",
    "plt.plot(l[:, 1].tolist(), l[:, 0].tolist(), color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oZ1BXWL7Mw16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501.18723362727246"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eyeball power, or computation power?\n",
    "sorted_data = sorted(return_data_list)\n",
    "sorted_data[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PcnSvUHyMw19"
   },
   "source": [
    "***QUIZ QUESTION***\n",
    "Also, using this value of L1 penalty, how many nonzero weights do you have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Od-dOA-IMw1-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interesting, isn't it\n",
    "best_model = sorted_data[0][2]\n",
    "np.count_nonzero(best_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_zNo9ywLMw2F"
   },
   "source": [
    "# Limit the number of nonzero weights\n",
    "\n",
    "What if we absolutely wanted to limit ourselves to, say, 5 features? This may be important if we want to derive \"a rule of thumb\" --- an interpretable model that has only a few features in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jLijk-3UMw2G"
   },
   "source": [
    "In this section, you are going to implement a simple, two phase procedure to achive this goal:\n",
    "1. Explore a large range of `l1_penalty` values to find a narrow region of `l1_penalty` values where models are likely to have the desired number of non-zero weights.\n",
    "2. Further explore the narrow region you found to find a good value for `l1_penalty` that achieves the desired sparsity.  Here, we will again use a validation set to choose the best value for `l1_penalty`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kOwQHIzMMw2H"
   },
   "outputs": [],
   "source": [
    "max_nonzeros = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KMv9UnluMw2N"
   },
   "source": [
    "## Exploring the larger range of values to find a narrow range with the desired sparsity\n",
    "\n",
    "Let's define a wide range of possible `l1_penalty_values`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vEaNt8VHMw2O"
   },
   "outputs": [],
   "source": [
    "l1_penalty_values = np.logspace(3, 5, num=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mxrj8371Mw2S"
   },
   "source": [
    "Now, implement a loop that search through this space of possible `l1_penalty` values:\n",
    "\n",
    "* For `l1_penalty` in `np.logspace(3, 5, num=21)`:\n",
    "    * Fit a regression model with a given `l1_penalty` on TRAIN data. Specify `alpha=l1_penalty` in the parameter.\n",
    "    * Extract the weights of the model and count the number of nonzeros. Save the number of nonzeros to a list.\n",
    "        * *Hint: `model.coef_` gives you the coefficients/parameters you learned (barring intercept) in the form of numpy array. You can then use array\\[condition\\] for the list of values passing the condition. Or just use the builtin `np.count_nonzero()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_kFzVsaiMw2U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=1000.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=1258.9254117941675, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=1584.893192461114, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=1995.2623149688789, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=2511.88643150958, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=3162.2776601683795, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=3981.0717055349733, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=5011.872336272725, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=6309.57344480193, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=7943.282347242814, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=10000.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=12589.254117941662, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=15848.93192461114, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=19952.62314968879, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=25118.864315095823, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=31622.776601683792, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=39810.71705534969, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=50118.72336272725, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=63095.7344480193, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=79432.82347242821, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Lasso(alpha=100000.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# Or you can do it with pure python. Its impact on the performance is negligible.\n",
    "return_data = []\n",
    "for l1_penalty in l1_penalty_values:\n",
    "    model = Lasso(alpha=l1_penalty).fit(X_train_features, y_train_label)\n",
    "#     y_predict = model.predict(X_validation_features)\n",
    "#     rss = np.sum(np.square(y_validation_label - y_predict))\n",
    "    num_nonzero = np.count_nonzero(model.coef_)\n",
    "    return_data.append([l1_penalty, num_nonzero])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ucgWXpj9Mw2Z"
   },
   "source": [
    "Out of this large range, we want to find the two ends of our desired narrow range of `l1_penalty`.  At one end, we will have `l1_penalty` values that have too few non-zeros, and at the other end, we will have an `l1_penalty` that has too many non-zeros.  \n",
    "\n",
    "More formally, find:\n",
    "* The smallest `l1_penalty` that has non-zeros equal `max_nonzeros` (if we pick a penalty smaller than this value, we will definitely have too many non-zero weights)\n",
    "    * Store this value in the variable `l1_penalty_min` (we will use it later)\n",
    "* The biggest `l1_penalty` that has non-zeros equal `max_nonzeros` (if we pick a penalty larger than this value, we will definitely have too few non-zero weights)\n",
    "    * Store this value in the variable `l1_penalty_max` (we will use it later)\n",
    "\n",
    "\n",
    "*Hint: there are many ways to do this, e.g.:*\n",
    "* Programmatically within the loop above\n",
    "* Creating a list with the number of non-zeros for each value of `l1_penalty` and inspecting it to find the appropriate boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 1000.0]\n",
      "[15, 1258.9254117941675]\n",
      "[15, 1584.893192461114]\n",
      "[15, 1995.2623149688789]\n",
      "[15, 2511.88643150958]\n",
      "[14, 3162.2776601683795]\n",
      "[13, 3981.0717055349733]\n",
      "[13, 5011.872336272725]\n",
      "[10, 6309.57344480193]\n",
      "[10, 7943.282347242814]\n",
      "[10, 10000.0]\n",
      "[10, 12589.254117941662]\n",
      "[6, 15848.93192461114]\n",
      "[6, 19952.62314968879]\n",
      "[5, 25118.864315095823]\n",
      "[5, 31622.776601683792]\n",
      "[5, 39810.71705534969]\n",
      "[5, 50118.72336272725]\n",
      "[4, 63095.7344480193]\n",
      "[3, 79432.82347242821]\n",
      "[2, 100000.0]\n"
     ]
    }
   ],
   "source": [
    "print(*return_data, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UBpwiXEVMw2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50118.72336272725 25118.864315095823\n"
     ]
    }
   ],
   "source": [
    "np_array = np.array(return_data)\n",
    "l1_penalty_max = np_array[np.where(np_array[:, 0] == max_nonzeros), 1][0, -1]\n",
    "l1_penalty_min = np_array[np.where(np_array[:, 0] == max_nonzeros), 1][0, 0]\n",
    "print(l1_penalty_max, l1_penalty_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-S2jtklaMw2g"
   },
   "source": [
    "\n",
    "***QUIZ QUESTION.*** What values did you find for `l1_penalty_min` and `l1_penalty_max`, respectively? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-T_xUbJ5Mw2i"
   },
   "source": [
    "## Exploring the narrow range of values to find the solution with the right number of non-zeros that has lowest RSS on the validation set \n",
    "\n",
    "We will now explore the narrow region of `l1_penalty` values we found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mEICQwrAMw2k"
   },
   "outputs": [],
   "source": [
    "l1_penalty_values_narrow = np.linspace(l1_penalty_min,l1_penalty_max,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IVkWedu9Mw2p"
   },
   "source": [
    "* For `l1_penalty` in `np.linspace(l1_penalty_min,l1_penalty_max,20)`:\n",
    "    * Fit a regression model with a given `l1_penalty` on TRAIN data. Specify `alpha=l1_penalty`.\n",
    "    * Measure the RSS of the learned model on the VALIDATION set\n",
    "\n",
    "Find the model that the lowest RSS on the VALIDATION set and has sparsity *equal* to `max_nonzeros`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKsCKTP3Mw2q"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for l1_penalty in l1_penalty_values_narrow: # for each l1 in list l1_penalty_values_narrow\n",
    "    model = Lasso(alpha=l1_penalty).fit(X_train_features, y_train_label)\n",
    "    y_predict = model.predict(X_validation_features)\n",
    "    rss = np.sum(np.square(y_validation_label - y_predict))\n",
    "    data.append([rss, l1_penalty, np.count_nonzero(model.coef_), model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b4vAuKXfMw2u"
   },
   "source": [
    "***QUIZ QUESTIONS***\n",
    "1. What value of `l1_penalty` in our narrow range has the lowest RSS on the VALIDATION set and has sparsity *equal* to `max_nonzeros`?\n",
    "2. What features in this model have non-zero coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VISUoAMIMw2v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer question 1: 25118.864315095823\n",
      "Answer question 2: ['sqft_living' 'waterfront' 'view' 'grade' 'yr_built']\n"
     ]
    }
   ],
   "source": [
    "print('Answer question 1: {}\\nAnswer question 2: {}'.\n",
    "      format(sorted(data)[0][1], np.array(all_features)[sorted(data)[0][3].coef_ != 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[493521711338201.5 25118.864315095823 5]\n",
      "[496574575559483.1 26434.64637023432 5]\n",
      "[499764416429739.25 27750.428425372815 5]\n",
      "[503090906690931.75 29066.210480511312 5]\n",
      "[506554043217954.94 30381.992535649806 5]\n",
      "[510153827525216.94 31697.774590788304 5]\n",
      "[513890260225477.94 33013.5566459268 5]\n",
      "[517763341318737.7 34329.338701065295 5]\n",
      "[521773070804996.1 35645.12075620379 5]\n",
      "[525919448684253.5 36960.90281134229 5]\n",
      "[530202474956509.6 38276.684866480784 5]\n",
      "[534622149621764.5 39592.466921619285 5]\n",
      "[539178473397453.6 40908.24897675778 5]\n",
      "[543871449941393.25 42224.03103189627 5]\n",
      "[548701075293986.5 43539.81308703477 5]\n",
      "[553667349455233.5 44855.59514217326 5]\n",
      "[558770272425134.4 46171.37719731176 5]\n",
      "[564009844203689.0 47487.15925245026 5]\n",
      "[569386064790897.4 48802.94130758876 5]\n",
      "[574898934186759.2 50118.72336272725 5]\n"
     ]
    }
   ],
   "source": [
    "np_data = np.array(sorted(data))\n",
    "print(*np_data[:,0:3], sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "lab-5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:ml-301x] *",
   "language": "python",
   "name": "conda-env-ml-301x-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
