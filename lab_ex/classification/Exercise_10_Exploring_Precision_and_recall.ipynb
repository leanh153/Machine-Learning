{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring precision and recall\n",
    "\n",
    "The goal of this second notebook is to understand precision-recall in the context of classifiers.\n",
    "\n",
    " * Use Amazon review data in its entirety.\n",
    " * Train a logistic regression model.\n",
    " * Explore various evaluation metrics: accuracy, confusion matrix, precision, recall.\n",
    " * Explore how various metrics can be combined to produce a cost of making an error.\n",
    " * Explore precision and recall curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some libs\n",
    "\n",
    "import pandas\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load amazon review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import amazon_baby.csv data to pandas dataframe\n",
    "products_df = pandas.read_csv('data/amazon_baby.csv')# Extract word counts and sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract word counts and sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the first assignment of this course, we compute the word counts for individual words and extract positive and negative sentiments from ratings. To summarize, we perform the following:\n",
    "\n",
    "1. Remove punctuation.\n",
    "2. Remove reviews with \"neutral\" sentiment (rating 3).\n",
    "3. Set reviews with rating 4 or more to be positive and those with 2 or less to be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    import string\n",
    "    return text.translate(string.punctuation)\n",
    "\n",
    "products_df = products_df.fillna({'review':''})  # fill in N/A's in the review column\n",
    "products_df['review_clean'] = products_df['review'].apply(remove_punctuation)\n",
    "\n",
    "products_df = products_df[products_df['rating'] != 3]\n",
    "\n",
    "products_df['sentiment'] = products_df['rating'].apply(lambda rating : +1 if rating > 3 else -1)# Extract word counts and sentimentsAs in the first assignment of this course, we compute the word counts for individual words and extract positive and negative sentiments from ratings. To summarize, we perform the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test sets\n",
    "\n",
    "We split the data into a 80-20 split where 80% is in the training set and 20% is in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "train_data, test_data = train_test_split(products_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature representation\n",
    " We will now compute the word count for each word that appears in the reviews. A vector consisting of word counts is often referred to as bag-of-word features. Since most words occur in only a few reviews, word count vectors are sparse. For this reason, scikit-learn and many other tools use sparse matrices to store a collection of word count vectors.\n",
    "<br>\n",
    "You can checkout the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from scikit documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "# Use this token pattern to keep single-letter words\n",
    "# First, learn vocabulary from the training data and assign columns to words\n",
    "# Then convert the training data into a sparse matrix\n",
    "train_matrix = vectorizer.fit_transform(train_data['review_clean'])\n",
    "# Second, convert the test data into a sparse matrix, using the same word-column mapping\n",
    "test_matrix = vectorizer.transform(test_data['review_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train logistic regression model\n",
    "Learn a logistic regression classifier using the training data. We are using scikit-learn, you should create an instance of the [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class and then call the method fit() to train the classifier. This model should use the sparse word count matrix (**train_matrix**) as features and the column **sentiment** of **train_data** as the target. Use the default values for other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tools\\Anaconda3\\envs\\machine-learning\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X, y = train_matrix, train_data['sentiment']\n",
    "X_test, y_test = test_matrix, test_data['sentiment']\n",
    "\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs').fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9475041416481136\n",
      "Test accuracy: 0.9344547389883362\n"
     ]
    }
   ],
   "source": [
    "print ('Train accuracy: {}'.format(clf.score(X, y)))\n",
    "print ('Test accuracy: {}'.format(clf.score(test_matrix, test_data['sentiment'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Majority class prediction\n",
    "\n",
    "Recall from an earlier assignment that we used the **majority class classifier** as a baseline (i.e reference) model for a point of comparison with a more sophisticated classifier. The majority classifier model predicts the majority class for all data points. \n",
    "\n",
    "Typically, a good model should beat the majority class classifier. Since the majority class in this dataset is the positive class (i.e., there are more positive than negative reviews), the accuracy of the majority class classifier can be computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy (majority class classifier): 0.8417438757458547\n"
     ]
    }
   ],
   "source": [
    "baseline = test_data[test_data['sentiment'] == 1].shape[0]/test_data.shape[0]\n",
    "print (\"Baseline accuracy (majority class classifier): %s\" % baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question:** Using accuracy as the evaluation metric, was our **logistic regression model** better than the baseline (majority class classifier)?\n",
    "<br>\n",
    "**Your answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "The accuracy, while convenient, does not tell the whole story. For a fuller picture, we turn to the **confusion matrix**. In the case of binary classification, the confusion matrix is a 2-by-2 matrix laying out correct and incorrect predictions made in each label as follows:\n",
    "```\n",
    "              +---------------------------------------------+\n",
    "              |                Predicted label              |\n",
    "              +----------------------+----------------------+\n",
    "              |          (+1)        |         (-1)         |\n",
    "+-------+-----+----------------------+----------------------+\n",
    "| True  |(+1) | # of true positives  | # of false negatives |\n",
    "| label +-----+----------------------+----------------------+\n",
    "|       |(-1) | # of false positives | # of true negatives  |\n",
    "+-------+-----+----------------------+----------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         pred:1  pred:-1\n",
      "true:1    27325      748\n",
      "true:-1    1438     3840\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "unique_label = [1, -1]\n",
    "confusion = confusion_matrix(y_test, y_test_pred, labels=unique_label)\n",
    "print(pandas.DataFrame(confusion, \n",
    "                       index=['true:{:}'.format(x) for x in unique_label], \n",
    "                       columns=['pred:{:}'.format(x) for x in unique_label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: How many predicted values in the **test set** are **false positives**?\n",
    "<br>\n",
    "**Your answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the cost of mistakes\n",
    "\n",
    "\n",
    "Put yourself in the shoes of a manufacturer that sells a baby product on Amazon.com and you want to monitor your product's reviews in order to respond to complaints.  Even a few negative reviews may generate a lot of bad publicity about the product. So you don't want to miss any reviews with negative sentiments --- you'd rather put up with false alarms about potentially negative reviews instead of missing negative reviews entirely. In other words, **false positives cost more than false negatives**. (It may be the other way around for other scenarios, but let's stick with the manufacturer's scenario for now.)\n",
    "\n",
    "Suppose you know the costs involved in each kind of mistake: \n",
    "1. \\$100 for each false positive.\n",
    "2. \\$1 for each false negative.\n",
    "3. Correctly classified reviews incur no cost.\n",
    "\n",
    "**Quiz Question**: Given the stipulation, what is the cost associated with the logistic regression classifier's performance on the **test set**?\n",
    "<br>\n",
    "**Your answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 144548$\n"
     ]
    }
   ],
   "source": [
    "FP, FN = confusion[1][0], confusion[0][1]\n",
    "cost = 100*FP + 1*FN\n",
    "\n",
    "print (\"cost: {}$\".format(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall\n",
    "You may not have exact dollar amounts for each kind of mistake. Instead, you may simply prefer to reduce the percentage of false positives to be less than, say, 3.5% of all positive predictions. This is where **precision** comes in:\n",
    "\n",
    "$$\n",
    "[\\text{precision}] = \\frac{[\\text{# positive data points with positive predicitions}]}{\\text{[# all data points with positive predictions]}} = \\frac{[\\text{# true positives}]}{[\\text{# true positives}] + [\\text{# false positives}]}\n",
    "$$\n",
    "\n",
    "So to keep the percentage of false positives below 3.5% of positive predictions, we must raise the precision to 96.5% or higher. \n",
    "\n",
    "**First**, let us compute the precision of the logistic regression classifier on the **test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "results = precision_recall_fscore_support(y_test, y_test_pred, labels=[1, -1])\n",
    "\n",
    "pricisions = results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95000522, 0.836966  ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pricisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: Out of all reviews in the **test set** that are predicted to be positive, what fraction of them are **false positives**? (Round to the second decimal place e.g. 0.25)\n",
    "<br>\n",
    "**Your answer**: 0.84\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positives percentage: 0.05\n"
     ]
    }
   ],
   "source": [
    "false_positives_per = FP / (FP + confusion[0][0])\n",
    "\n",
    "print ('false positives percentage: {0:.2f}'.format(false_positives_per))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A complementary metric is **recall**, which measures the ratio between the number of true positives and that of (ground-truth) positive reviews:\n",
    "\n",
    "$$\n",
    "[\\text{recall}] = \\frac{[\\text{# positive data points with positive predicitions}]}{\\text{[# all positive data points]}} = \\frac{[\\text{# true positives}]}{[\\text{# true positives}] + [\\text{# false negatives}]}\n",
    "$$\n",
    "\n",
    "Let us compute the recall on the **test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97335518 0.72754831]\n"
     ]
    }
   ],
   "source": [
    "recalls = results[1]\n",
    "print (recalls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision-recall tradeoff\n",
    "\n",
    "In this part, we will explore the trade-off between precision and recall discussed in the lecture.  We first examine what happens when we use a different threshold value for making class predictions.  We then explore a range of threshold values and plot the associated precision-recall curve.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_threshold(probabilities, threshold):\n",
    "    results = np.array(len(probabilities)*[1])\n",
    "    results[probabilities > threshold] = 1\n",
    "    results[probabilities <= threshold] = 0\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run prediction with `output_type='probability'` to get the list of probability values. Then use thresholds set at 0.5 (default) and 0.9 to make predictions from these probability values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "Number of positive predicted reviews (threshold = 0.5): 4588\n",
      "Number of positive predicted reviews (threshold = 0.9): 2529\n"
     ]
    }
   ],
   "source": [
    "probabilities = clf.predict_proba(X_test)[:, 0]\n",
    "predictions_with_default_threshold = apply_threshold(probabilities, 0.5)\n",
    "predictions_with_high_threshold = apply_threshold(probabilities, 0.9)\n",
    "\n",
    "print (predictions_with_default_threshold)\n",
    "print (\"Number of positive predicted reviews (threshold = 0.5): %s\" % (predictions_with_default_threshold == 1).sum())\n",
    "print (\"Number of positive predicted reviews (threshold = 0.9): %s\" % (predictions_with_high_threshold == 1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-recall curve\n",
    "\n",
    "Now, we will explore various different values of tresholds, compute the precision and recall scores, and then plot the precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "thresold = [0.1, 0.5, 0.9]\n",
    "precision, recall, _ = precision_recall_curve(y_test, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEGCAYAAAB7DNKzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAdNklEQVR4nO3de5BU53nn8e8zw4wAAxJohCwBMleBKcVXVpbLtYlStlWSayO2Ui5HSly2stqw8VqxEzve9Va2bK2yW5vI8WaTWBuHRIoU1cay7Kp4iRdHtrWKsWSBGARIgECMEIJhJEYgmDs9M93P/vH2cfcMM91nhj59+vL7VHXRp/t0z8MpOL95z3s55u6IiIhMpyXtAkREpLYpKEREpCQFhYiIlKSgEBGRkhQUIiJS0py0C5ipjo4OX7lyZdpliIjUlT179pxx96tm89m6C4qVK1fS2dmZdhkiInXFzF6b7Wd16UlEREpSUIiISEkKChERKUlBISIiJSkoRESkJAWFiIiUlFhQmNlDZtZrZgemed/M7M/NrMvMXjCz9yVVi4iIzF6SLYqHgVtLvH8bsC7/2AL8ZYK1iIjILCUWFO6+A3irxC6bgb/zYCdwhZldU+57h4YqVaGIiMSRZh/FMuBk0XZ3/rWLmNkWM+s0s84zZ86RyVSlPhERId2gsClem/J2e+6+1d03ufumRYsWo5vyiYhUT5pB0Q2sKNpeDvSkVIuIiEwjzaDYBnwqP/rpJqDP3V9PsR4REZlCYqvHmtm3gJuBDjPrBr4KtAG4+zeB7cDHgC5gGPjNpGoREZHZSywo3P3OMu878Nmkfr6IiFSGZmaLiEhJCgoRESlJQSEiIiUpKEREpCQFhYiIlKSgEBGRkhQUIiJSkoJCRERKUlCIiEhJCgoRESlJQSEiIiUpKEREpCQFhYiIlKSgEBGRkhQUIiJSkoJCRERKUlCIiEhJCgoRESlJQSEiIiUpKEREpCQFhYiIlKSgEBGRkhQUIiJSkoJCRERKUlCIiEhJc9IuQEREkjE+XnhcCgWFiEidyGZhbCyc+Cf/GT36+qCrK+w7Udusz/cKChGRFEUn++ITf/Hz0VF4+WUYHgb3wufcQxhMDgSzidvu0efmz5ttjQoKEZEKy+UKJ/ziR3Tyz2Tg4MHwvPjkH534i18rPvFHr7tDS0t4tLcX9lmxAhYsuLiesbHiT8+cgkJEJKbiABgdLZz8o+d9fXD0aNgvMvk3f7PCib34xB+9N2dOCACARYvg2msvbiVUm4JCRISJJ/zJfw4MwOHDEwMglysEQHQin/zbf/Sbf2srXHZZYZ9168Jr9UJBISINb3IIFD9On4aTJy++BBSNFJrqt/lcrhAAbW3htcWL4e1vT/7vkoZEg8LMbgX+DGgF/sbd/2jS+9cBjwBX5Pf5srtvT7ImEWks7hef/KNHXx8cOTKzEHAPARBd+29vh9Wr07/8k6bEgsLMWoEHgI8C3cBuM9vm7oeKdvvPwOPu/pdmthHYDqxMqiYRqT+5XOHEn8lMDILXXoPe3sK+UQhM1QEcKb4M1NEBV12V/N+h3iXZorgR6HL3YwBm9hiwGSgOCgcW5Z9fDvQkWI+I1KCoRRCFQPGf+/dHI3aC6YaDFvcHtLWFP81g/frmbglUSpJBsQw4WbTdDXxg0j73Aj80s98B3gZ8ZKovMrMtwBaAjo7VFS9URJKVy4UT/+THG2/AiROF3/onB0EUAlC4JNTWFl5v5D6BWpNkUEyV45PH8d4JPOzuXzezDwKPmtkN7p6b8CH3rcBWgDVrNs16LLCIJCcKgwsXJobBK6/A2bNhn+Khoi0tYbu4RVAcBNdcA1dcke7fSYIkg6IbWFG0vZyLLy3dDdwK4O7PmtlcoAPoRURq0uhoCIPokcmEUUM9+f/d7qGfIBoZFL0WmTMnPAA2bNCloXqQZFDsBtaZ2SrgFHAH8OuT9jkBfBh42MzeCcwF3kywJhGJwX1iGIyMhCUk9u+Pd5lozpzQMpgzJ8wZkPqWWFC4+7iZ3QM8QRj6+pC7HzSz+4BOd98GfBH4azP7PcJlqbvcZz/NXERmJpcrBEEUCmfPhtnF0eWgqDO5eDZx8RBSUMug0SU6jyI/J2L7pNe+UvT8EPChJGsQkUIgRKEwMhKGlR47Vnh/qrkF7oUlJZYtC0tKSPPRzGyRBpPJFMJgeBjeeitMOoOJLQSYeLko6kRet67QhyACCgqRupXNFsIgCobnny+sR1QcCFAYYaRAkJnSPxOROjA+DkNDIRSGh+HUqTD/AEJg5HIT98/lChPPVq6EebO+E4GIgkKk5kwOhZMnobu78F7x3AMotBIA3vnOdGqWxqagEElRLhfCYGgoPE6fDhPU4OJQKO5YXr26sF6RSNIUFCJVlMnA4GAIhcFB2Lu3MFs5lyuMOMrlFApSOxQUIgkpbi0MDoYb35w7VwiGybe1bGvTshVSmxQUIhWSzYZAiB6dnRe3FopnLpupT0Hqg4JCZJbGx0MgDAyEx/PPF14v7luIWgu6hCT1SkEhElM2WwiFvj7Yty+8Hs1ojloLra0hHNRakEahoBCZRi5XCIaBAdizJ7weBQMURiLpMpI0MgWFSJ576Hju7w+PPXsKS2ZH74OCQZqPgkKaWiZTCIadO6fufNalJGl2CgppKtHlpP7+sJz2gQOFVkM0hyHqfFYwiAQKCml4Uauhrw927Zp4OQnCdlsbrFoFc+emV6dIrVJQSMOJ+hrOnw/3XDh06OJw0OUkkfgUFNIQstnQYjh/PvQ15HLhtdHRwlLabW2wZk3hrmwiEo+CQurW+HgIhvPnL76kFHVCz5unVoPIpVJQSF2JwuHcOXjuucItPFtaCnMaOjrgqqvSrlSkcSgopOZls4Vw2LWrcFkpGr6qjmiRZCkopCblcmGk0pkzoc8BJg5hnTMH1q4t3LBHRJKjoJCaMjgY5jc891wY1jo2FgKirS2Ew/LlsHBh2lWKNBcFhaRubCyEQ28vvPBCuKwEhT6H5cthyZJ0axRpZgoKSYV76Hc4cQJefDFs53LhPTPNcRCpJQoKqapMJtwX+vDh0ILIZEKrIWo9KBxEao+CQhLnHjqm33wzdExHtwE1C5Pfli2Dyy9Pu0oRmY6CQhIzPh76Hk6dCstoRENaQa0HkXqioJCKGxkJl5eefnpi30NLC1x7LVxxRbr1icjMKCikYvr74fjxwsilbDYso9HaqtaDSD1TUMglcQ8zpk+fDneEy2TCnIeWFgWESKNQUMis5HJh1vTx4/Dyy4XLS+3tYd7DokWpliciFRQ7KMxsGfCO4s+4+44kipLalc2GYa0HDoRWRDQ5rqUFNmwIf4pIY4kVFGb2x8CvAYeA/KkBB0oGhZndCvwZ0Ar8jbv/0RT7fAK4N/99+9391+MWL9UTBcSzz4abArmH13V5SaTxxW1R/Gtgvbtn4n6xmbUCDwAfBbqB3Wa2zd0PFe2zDvhPwIfc/ZyZLY1fulTD+HhoOfz0p2GpjSggNHNapHnEDYpjQBsQOyiAG4Eudz8GYGaPAZsJrZLIbwEPuPs5AHfvncH3S4JyuRAQO3aEDmrQ0hoizSpuUAwD+8zsSYrCwt0/V+Izy4CTRdvdwAcm7XM9gJk9Q7g8da+7/1PMmiQBuVyYQb1/fwgKCAFhpoAQaVZxg2Jb/jETNsVrPsXPXwfcDCwHfmpmN7j7+QlfZLYF2ALQ0bF6hmVIHO5hFNORI2EkUy5XuJ2oAkKkucUKCnd/xMzaybcAgCPuPlbmY93AiqLt5UDPFPvszH/Xq2Z2hBAcuyf9/K3AVoA1azZNDhu5RH19YZmN55+fOItaASEiEH/U083AI8BxQkthhZl9uszw2N3AOjNbBZwC7gAmj2j6HnAn8LCZdRCC6NhM/gIyewMD0NMDu3erBSEi04t76enrwC3ufgTAzK4HvgW8f7oPuPu4md0DPEHof3jI3Q+a2X1Ap7tvy793i5lFw26/5O5nZ//XkThGR8N9IHbuDCOZWltDH8T69WGxPhGRYnFPC21RSAC4+8tmVvZuxe6+Hdg+6bWvFD134Av5hyTMHV57LSzWl82G0Uzz5sHSpdDRkXZ1IlKr4gZFp5k9CDya3/4NYE8yJUkSBgbg1VcLC/a1tMD8+brMJCLlxQ2KzwCfBT5H6KPYAfyvpIqSyslmobs7TJhzVz+EiMxc3FFPGeB/5B9SJ956K7QgTpwo3DBI/RAiMlMlTxlm9ri7f8LMXuTiORC4+7sSq0xmbXQ09EXs2lUY7qoJcyIyW+V+t/x8/s9/lXQhUhm9vfDDHxb6Icxg48a0qxKRelYyKNz99fzTM8CIu+fyQ2M3AD9IujiJb2wsdFY/9xwMD8PcuWpFiEhlxL1avQP4l2a2GHgS6CQsO/4bSRUm8b35ZpgTce5cCAeNZhKRSoobFObuw2Z2N/AX7n6/me1NsjApb2wMjh6duPSGWhEiUmmxg8LMPkhoQdw9w89KAgYGYN8+OHasMHFOfREikoS4J/vfJdxg6B/yy3CsBp5KriyZjnvoi/jZz8JzTZwTkaTFnUfxE+AnRdvHCJPvpIrGxuDgwXC/6mhUk+5TLSJJKzeP4n+6+++a2T8y9TyK2xOrTCYYGAirvJ48GQJj7ly1IkSkOsq1KKK1nf4k6UJkeqdOwVNPhQ7rlhaFhIhUV7l5FNHCf53k51EAmFkrcFnCtTW9sbEww3rnTujvh4ULQ0DYVPcOFBFJSNyr208C84u25wE/rnw5EhkeDpPnok7rRYvCqCaFhIhUW9xRT3PdfTDacPdBM5tf6gMye3198P3vhzWbzMIifrrUJCJpiRsUQ2b2Pnd/HsDM3g+MJFdW8zp9Gn70o9Af0doKa9aEPgkRkbTMZB7Fd8ysJ799DWEJD6kQ9zCiaceO8FyL+YlIrYg7j2K3mW0A1hNuXHTY3ccSrayJ5HJw/HihP0LLcIhILYnVmZ3vj/iPwOfd/UVgpZlp6fEKyOWgqwueeUYhISK1Ke6op78FRoEP5re7gf+aSEVNZHwcjhwJNxgyU0iISG2KGxRr3P1+YAzA3UcIl6BklsbG4PBh6OwsvKaQEJFaFLcze9TM5pFfxsPM1gCZxKpqcKOjIST27VNLQkRqX9yg+CrwT8AKM/vfwIeAu5IqqpFlMvDii/DSSyEgWlth/fq0qxIRmV7ZoDAzAw4DvwrcRLjk9Hl3P5NwbQ3nwoXQiujqCtvz58PKlamWJCJSVtmgcHc3s++5+/uB/1uFmhrSyEjotD55Mmx3dMDSpenWJCISR9zO7J1m9i8SraSBDQ2F2dZRSCxbppAQkfoRt4/il4HfNrPjwBDh8pO7+7uSKqxRDA/Dj38cVn8FuO46WLAg3ZpERGYiblDclmgVDaqvL1xu6u8PHdfr1oUF/kRE6km5O9zNBX4bWAu8CDzo7uPVKKzeZTLwwgvQ0xNGNq1erZAQkfpUro/iEWATISRuA76eeEUNILq39bFjoSWxbBlcpts8iUidKvc77kZ3/wUAM3sQeC75kupbNgtHj8KBAyEkrr4aLr887apERGavXIvi5yvE6pJTee5hjsSePSEk2trCMFgRkXpWLijebWb9+ccA8K7ouZn1l/tyM7vVzI6YWZeZfbnEfh83MzezTTP9C9SSw4fD7UshBMX116dbj4hIJZS89OTurbP9YjNrBR4APkpYbXa3mW1z90OT9lsIfA7YNdufVQtefTW0JLJZ3bpURBpL3Al3s3Ej0OXux9x9FHgM2DzFfn8I3A9cSLCWRJ07F+4n0denkBCRxpNkUCwDThZtd+df+zkzey+wwt2/X+qLzGyLmXWaWWd//7nKV3oJzp+HJ54I6zgtWqSQEJHGk2RQTHW/Cv/5m2YtwJ8CXyz3Re6+1d03ufumRYsWV7DES3PhQhjdNDwchr/qHtci0oiSDIpuYEXR9nKgp2h7IXAD8M/5pUFuArbVS4d2LhfmSrzyShjdtHp12hWJiCQjyaDYDawzs1Vm1g7cAWyL3nT3PnfvcPeV7r4S2Anc7u6dU39dbTlxAg4dCoHR0gJz56ZdkYhIMhILivy8i3uAJ4CXgMfd/aCZ3Wdmtyf1c6vhzTfh6afDvIm2NtiwIe2KRESSk+jqQ+6+Hdg+6bWvTLPvzUnWUilDQ6HzOpOB9nZ1XotI40vy0lPDGR8PnddjY6Eloc5rEWkGCooZ2L8fjhwJs65XrUq7GhGR6lBQxHT6NLz0EgwOhkl18+enXZGISHUoKGKI7lLnrkl1ItJ8FBRl5HLhvhK5XAgKhYSINBvdc62MAwdC3wTAmjXp1iIikga1KEo4ezbczjSTCR3YmlQnIs1IQTGNsbGwbPjISAgIXXISkWaloJjGyZPw+uthsT+FhIg0M/VRTOH8efjZz8IaTjbVGrgiIk1ELYpJstnQgZ3LhW21JkSk2SkoJjl6NNzWFLREh4gIKCgmOH8eOjthdDRcdhIREfVRAPDGG2Eo7PnzYbSTVoUVESlQUABvvQW7doWZ1+3tsH592hWJiNQOBQWh49oM3v3utCsREak9uhIPXLgQHiIicrGmD4rBQTh8uDAcVkREJmrqoHAPNyIaHQ3Lh4uIyMWaOihefz3cjEh3rBMRmV7TBkU2G1oTQ0Ph/tfz5qVdkYhIbWraoOjqgu7uEBCaMyEiMr2mDIrRUdi9O7QmNANbRKS0pjxNnjoVRjnNn6/WhIhIOU0XFEND8Mwz4blaEyIi5TXdqbKnJ3Rkm6k1ISISR1MFxfnzYU2nbBauvjrtakRE6kNTBcUrr4TVYefMgSuvTLsaEZH60DRBMTAQJtdduADXXpt2NSIi9aNpguLll0NrYsECWLw47WpEROpHUwTF6dOhNTE0BG9/e9rViIjUl6YIiu5uyGTCwn9LlqRdjYhIfUk0KMzsVjM7YmZdZvblKd7/gpkdMrMXzOxJM3tHpWuIFv4bH4e1ayv97SIijS+xoDCzVuAB4DZgI3CnmW2ctNteYJO7vwv4LnB/pevo6QlLdsybB5ddVulvFxFpfEm2KG4Eutz9mLuPAo8Bm4t3cPen3H04v7kTWF7JAqKRTn19sGJFJb9ZRKR5JBkUy4CTRdvd+demczfwg6neMLMtZtZpZp39/edwj1fAyy+HyXVLlujGRCIis5VkUNgUr015ijezTwKbgK9N9b67b3X3Te6+adGixWQy5X/40FBoTQwMQGvrDKoWEZEJ5iT43d1A8QWf5UDP5J3M7CPAHwC/5O4xIiCeN94IrYkFC7Smk4jIpUiyRbEbWGdmq8ysHbgD2Fa8g5m9F/gr4HZ3763UD85m4ejR8OecJKNQRKQJJBYU7j4O3AM8AbwEPO7uB83sPjO7Pb/b14AFwHfMbJ+ZbZvm62akpwfOnNEKsSIilZDo79vuvh3YPum1rxQ9/0gSP7e3F4aHw42JRETk0jTczOxz50IndkuLWhMiIpXQcEFx7Fjom2hvT7sSEZHG0FBBEQ2J7e/XkFgRkUppqKA4fBhyudA3sWFD2tWIiDSGhgqK0dHQqtCaTiIildMwQdHbG2512t6uTmwRkUpqmKA4eTK0KNra0q5ERKSxNERQFK/rdNVVaVcjItJYGiIook7shQth6dK0qxERaSwNERRjYzA4qLkTIiJJqPugOHsWurrC4n/qxBYRqby6D4rjx8P9sDUkVkQkGXUdFLlcmIU9OKiZ2CIiSanroDh1KjxaW3XZSUQkKXUdFL29kMmEu9iJiEgy6jYo3MPtToeGYNWqtKsREWlcdRsUvb3h3hNz58K8eWlXIyLSuOo2KLq7NdpJRKQa6jYocrlwu1ONdhIRSVZdBsXwMBw5AiMjGu0kIpK0ugyKnp7QoliyJO1KREQaX10GxeAgXLigJcVFRKqh7oLCPXRkj4zA2rVpVyMi0vjqMiggDItVi0JEJHl1FxQA2ayGxYqIVEtdBsWFC2FZcRERSV7dBsWaNWlXISLSHOoyKMx0NzsRkWqpy6CIOrRFRCR5dRcU7vC2t6VdhYhI86jLoJg7N+0qRESaR90FhYiIVFddBoWGxoqIVE+iQWFmt5rZETPrMrMvT/H+ZWb27fz7u8xsZbnvdIcrr0yiWhERmUpiQWFmrcADwG3ARuBOM9s4abe7gXPuvhb4U+CP43y3ZmWLiFRPkhdxbgS63P0YgJk9BmwGDhXtsxm4N//8u8A3zMzcSw+AzWR0wyIRkbhGRwFs1p9PMiiWASeLtruBD0y3j7uPm1kfcCVwpngnM9sCbMlvjd1yy6JXNJsCYGwxtJ1Lu4raoGNRoGNRoGMRmMHgdbP9dJJBMVV8TT65x9kHd98KbAUws073/k2XXl79C8figo4FOhbFdCwKdCwKzKxztp9NsjO7G1hRtL0c6JluHzObA1wOvJVgTSIiMkNJBsVuYJ2ZrTKzduAOYNukfbYBn84//zjw/8r1T4iISHUldukp3+dwD/AE0Ao85O4Hzew+oNPdtwEPAo+aWRehJXFHjK/emlTNdUjHokDHokDHokDHomDWx8L0C7yIiJRSlzOzRUSkehQUIiJSUs0GRRLLf9SrGMfiC2Z2yMxeMLMnzewdadRZDeWORdF+HzczN7OGHRoZ51iY2Sfy/zYOmtnfV7vGaonxf+Q6M3vKzPbm/598LI06k2ZmD5lZr5kdmOZ9M7M/zx+nF8zsfbG+2N1r7kHo/H4FWA20A/uBjZP2+ffAN/PP7wC+nXbdKR6LXwbm559/ppmPRX6/hcAOYCewKe26U/x3sQ7YCyzOby9Nu+4Uj8VW4DP55xuB42nXndCx+EXgfcCBad7/GPADwhy2m4Bdcb63VlsUP1/+w91HgWj5j2KbgUfyz78LfNjMZj9HvXaVPRbu/pS7D+c3dxLmrDSiOP8uAP4QuB+4UM3iqizOsfgt4AF3Pwfg7r1VrrFa4hwLBxbln1/OxXO6GoK776D0XLTNwN95sBO4wsyuKfe9tRoUUy3/sWy6fdx9HIiW/2g0cY5FsbsJvzE0orLHwszeC6xw9+9Xs7AUxPl3cT1wvZk9Y2Y7zezWqlVXXXGOxb3AJ82sG9gO/E51Sqs5Mz2fAMku4XEpKrb8RwOI/fc0s08Cm4BfSrSi9JQ8FmbWQliF+K5qFZSiOP8u5hAuP91MaGX+1MxucPfzCddWbXGOxZ3Aw+7+dTP7IGH+1g3unku+vJoyq/NmrbYotPxHQZxjgZl9BPgD4HZ3z1SptmordywWAjcA/2xmxwnXYLc1aId23P8j/8fdx9z9VeAIITgaTZxjcTfwOIC7PwvMBTqqUl1tiXU+maxWg0LLfxSUPRb5yy1/RQiJRr0ODWWOhbv3uXuHu69095WE/prb3X3Wi6HVsDj/R75HGOiAmXUQLkUdq2qV1RHnWJwAPgxgZu8kBMWbVa2yNmwDPpUf/XQT0Ofur5f7UE1eevLklv+oOzGPxdeABcB38v35J9z99tSKTkjMY9EUYh6LJ4BbzOwQkAW+5O5n06s6GTGPxReBvzaz3yNcarmrEX+xNLNvES41duT7Y74KtAG4+zcJ/TMfA7qAYeA3Y31vAx4rERGpoFq99CQiIjVCQSEiIiUpKEREpCQFhYiIlKSgEBGRkhQUIpOYWdbM9pnZATP7RzO7osLff5eZfSP//F4z+/1Kfr9IpSkoRC424u7vcfcbCHN0Ppt2QSJpUlCIlPYsRYummdmXzGx3fi3//1L0+qfyr+03s0fzr/1K/l4pe83sx2Z2dQr1i1yympyZLVILzKyVsOzDg/ntWwhrJd1IWFxtm5n9InCWsM7Wh9z9jJktyX/F08BN7u5m9m+B/0CYISxSVxQUIhebZ2b7gJXAHuBH+ddvyT/25rcXEILj3cB33f0MgLtHi1MuB76dX++/HXi1KtWLVJguPYlcbMTd3wO8g3CCj/ooDPjv+f6L97j7Wnd/MP/6VGvh/AXwDXf/BeDfERaiE6k7CgqRabh7H/A54PfNrI2w6Ny/MbMFAGa2zMyWAk8CnzCzK/OvR5eeLgdO5Z9/GpE6pUtPIiW4+14z2w/c4e6P5peofja/Su8g8Mn8SqX/DfiJmWUJl6buItxV7Ttmdoqw5PmqNP4OIpdKq8eKiEhJuvQkIiIlKShERKQkBYWIiJSkoBARkZIUFCIiUpKCQkRESlJQiIhISf8feDf5Et8d3mQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from inspect import signature\n",
    "\n",
    "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machine-learning] *",
   "language": "python",
   "name": "conda-env-machine-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
